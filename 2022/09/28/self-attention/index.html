<!DOCTYPE html>
<html lang="zh-Hansn">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/avatar.png">
  <link rel="icon" type="image/png" href="/img/avatar.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="description" content="">
  <meta name="author" content="JiaoPan">
  <meta name="keywords" content="deep learning|computer vision">
  <title>self-attention|transformer ~ 今天的风儿甚是喧嚣 JiaoPan</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>今天的风儿甚是喧嚣 JiaoPan</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/default.png')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  Wednesday, September 28th 2022, 9:14 pm
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    2.2k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      12 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h3 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h3><script type="math/tex; mode=display">X = \begin{bmatrix}x_{1} \\ x_{2} \\x_{3} \end{bmatrix}=\begin{bmatrix}a_{11}&a_{12}&a_{13}&a_{14} \\ a_{21}&a_{22}&a_{23}&a_{24} \\ a_{31}&a_{32}&a_{33}&a_{34} \end{bmatrix}=\begin{bmatrix}1&2&1&2 \\ 1&1&2&2 \\ 1&1&1&-1 \end{bmatrix}</script><script type="math/tex; mode=display">x_i = [a_{i1},a_{i2},a_{i3},a_{i4}]</script><p>词向量: $x_1$(早)，$x_3$(上)，$x_3$(好)</p>
<hr>
<script type="math/tex; mode=display">attention = softmax(XX^T)X</script><script type="math/tex; mode=display">XX^T = \begin{bmatrix}a_{11}&a_{12}&a_{13}&a_{14} \\ a_{21}&a_{22}&a_{23}&a_{24} \\ a_{31}&a_{32}&a_{33}&a_{34} \end{bmatrix} \begin{bmatrix}a_{11}&a_{21}&a_{31} \\ a_{12}&a_{22}&a_{32} \\ a_{13}&a_{23}&a_{33} \\ a_{14}&a_{24}&a_{34} \end{bmatrix}=\begin{bmatrix}10&9&2 \\ 9&10&2 \\ 2&2&4 \end{bmatrix}</script><p>$x_ix_i^T$ : <strong>表征两个向量的夹角,表征一个向量在另一个向量上的投影,投影的值大,说明两个向量相关度高</strong></p>
<p><strong>归一化 -&gt; 权重</strong></p>
<script type="math/tex; mode=display">softmax(XX^T) = \frac{e^z_i}{\sum_{i=0}e^z_i} = \begin{bmatrix}0.476&0.428&0.096 \\ 0.428&0.476&0.096 \\ 0.25&0.25&0.5 \end{bmatrix}(Z)</script><p>$z_1$(早) = $[0.476,0.428,0.096]$<br>即: 分配0.476权重给予本身 , 0.428的权重给予”上” , 0.096的权重给予”上”</p>
<p><strong>加权求和 -&gt; attention向量</strong></p>
<script type="math/tex; mode=display">softmax(XX^T)X 

= ZX</script><script type="math/tex; mode=display">=\begin{bmatrix}0.476&0.428&0.096 \\ 0.428&0.476&0.096 \\ 0.25&0.25&0.5 \end{bmatrix}\begin{bmatrix}1&2&1&2 \\ 1&1&2&2 \\ 1&1&1&-1 \end{bmatrix}=\begin{bmatrix}1&1.476&1.428&1.714\\ 1&1.428&1.476&1.714 \\ 1&1.25&1.25&0.5 \end{bmatrix}</script><hr>
<script type="math/tex; mode=display">attention = softmax(\frac{QK^T}{\sqrt{d_k}})V=softmax(\frac{Q_{n \times d_k}K^T_{d_k \times n}}{\sqrt{d_k}})V_{n \times d_v}=O_{n \times n}V_{n \times d_v}</script><p>$n$ : 输入词向量个数</p>
<p>$d$ : dimension of input,输入向量维度，d = dimension of X = 4</p>
<p>$d_k$ : dimension of k ,即<strong>K</strong>的行维度，可取 = $d$ = dimension of $x$</p>
<p>$d_v$ : dimension of v ,即<strong>V</strong>的行维度，可取 = $d$ = dimension of $x$</p>
<script type="math/tex; mode=display">Q_{n \times d_k}=X_{n \times d}W^q_{d \times d_k}</script><script type="math/tex; mode=display">K_{n \times d_k}=X_{n \times d}W^k_{d \times d_k}</script><script type="math/tex; mode=display">V_{n \times d_k}=X_{n \times d}W^v_{d \times d_k}</script><p> ($W^q,W^k,W^v$ 变换矩阵,可学习参数)</p>
<p><img src="/images/self-attention02.jpg" srcset="/img/loading.gif" alt=""><br><img src="/images/self-attention03.jpg" srcset="/img/loading.gif" alt=""></p>
<hr>
<h4 id="self-attention-pytorch-Implement"><a href="#self-attention-pytorch-Implement" class="headerlink" title="self-attention pytorch Implement"></a>self-attention pytorch Implement</h4><pre><code class="lang-python">import torch  
from torch import nn  
import numpy as np  

class SelfAttention(nn.Module):  
    def __init__(self,input_dim,key_dim=None,value_dim=None):  
        super(SelfAttention, self).__init__()  

        self.input_dim = input_dim  
        if key_dim is None:  
            self.key_dim = input_dim  
        if value_dim is None:  
            self.value_dim = input_dim  

        self.query = nn.Linear(self.input_dim,key_dim,bias=False)  
        self.key = nn.Linear(self.input_dim, key_dim, bias=False)  
        self.value = nn.Linear(self.input_dim, value_dim, bias=False)  
        self.norm_fact = 1 / np.sqrt(key_dim)  

    # X-&gt;[batch_size ,seq_len,input_dim]  
    def forward(self,X):  
        query = self.query(X)  
        key = self.key(X)  
        value = self.value(X)  
        keyT = key.permute(0, 2, 1) # key[batch_size ,seq_len,input_dim] -&gt; [batch_size,input_dim,seq_len]  
        attention = nn.Softmax(dim=-1)(torch.bmm(query,keyT)) * self.norm_fact  
        output = torch.bmm(attention,value)  

        return output  

model = SelfAttention(4,5,3)  
X = torch.rand(1,3,4)  
print(X)  
output = model(X)  
print(output)  
print(output.size())
</code></pre>
<pre><code class="lang-bash">tensor([[[0.9641, 0.7163, 0.9725, 0.6660],
         [0.2968, 0.2546, 0.8528, 0.9114],
         [0.0058, 0.5362, 0.1145, 0.9804]]])
tensor([[[-0.0294,  0.1206,  0.1320],
         [-0.0314,  0.1191,  0.1306],
         [-0.0329,  0.1189,  0.1299]]], grad_fn=&lt;BmmBackward0&gt;)
torch.Size([1, 3, 3])
</code></pre>
<hr>
<p><strong>Multi-head Self-Attention</strong></p>
<script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head_1,head_2,...,head_h)W^o$$$$head_i = attention(Q_i,K_i,V_i)</script><script type="math/tex; mode=display">Q,K,V - [heads,seq·len,\frac{dim}{heads}]</script><script type="math/tex; mode=display">Q_i,K_i,V_i:[{ seqlen,\frac{dim}{heads}}],W^o:W^o_{dim \times dim}</script><p><img src="/images/self-attention04.jpg" srcset="/img/loading.gif" alt=""><br><img src="/images/self-attention06.jpg" srcset="/img/loading.gif" alt=""><br><img src="/images/self-attention07.jpg" srcset="/img/loading.gif" alt=""></p>
<p>$heads=2,dim=4$,拆分向量维度值head_dim = dim / heads = 2</p>
<script type="math/tex; mode=display">X_{seqlen \times dim}=\begin{bmatrix}x_{11}&x_{12}&x_{13}&x_{14} \\ x_{21}&x_{22}&x_{23}&x_{24} \\ x_{31}&x_{32}&x_{33}&x_{34} \end{bmatrix}</script><script type="math/tex; mode=display">W^{Q,K,V}_{dim \times dim}=\begin{bmatrix}w_{11}&w_{12}&w_{13}&w_{14} \\ w_{21}&w_{22}&w_{23}&w_{24} \\ w_{31}&w_{32}&w_{33}&x_{34} \\ w_{41}&w_{42}&w_{43}&w_{44} \end{bmatrix}</script><p>shift $XW:[seqlen,dim]-&gt;[heads,seqlen,\frac{dim}{heads}]$</p>
<script type="math/tex; mode=display">Q'=XW^Q=\begin{bmatrix}q_{11}&q_{12}&q_{13}&q_{14} \\ q_{21}&q_{22}&q_{23}&q_{24} \\ q_{31}&q_{32}&q_{33}&q_{34} \end{bmatrix},Q=[[Q'_1],[Q'_2]]=\begin{bmatrix}\begin{bmatrix}q_{11}&q_{12}\\ q_{21}&q_{22} \\ q_{31}&q_{32}\end{bmatrix},\begin{bmatrix}q_{13}&q_{14} \\ q_{23}&q_{24} \\ q_{33}&q_{34} \end{bmatrix}\end{bmatrix}</script><script type="math/tex; mode=display">K'=XW^K=\begin{bmatrix}k_{11}&k_{12}&k_{13}&k_{14} \\ k_{21}&k_{22}&k_{23}&k_{24} \\ k_{31}&k_{32}&k_{33}&k_{34} \end{bmatrix},K=[[K'_1],[K'_2]]=\begin{bmatrix}\begin{bmatrix}k_{11}&k_{12}\\ k_{21}&k_{22} \\ k_{31}&k_{32}\end{bmatrix},\begin{bmatrix}k_{13}&k_{14} \\ k_{23}&k_{24} \\ k_{33}&k_{34} \end{bmatrix}\end{bmatrix}</script><script type="math/tex; mode=display">V'=XW^V=\begin{bmatrix}v_{11}&v_{12}&v_{13}&v_{14} \\ v_{21}&v_{22}&v_{23}&v_{24} \\ v_{31}&v_{32}&v_{33}&v_{34} \end{bmatrix},V=[[V'_1],[V'_2]]=\begin{bmatrix}\begin{bmatrix}v_{11}&v_{12}\\ v_{21}&v_{22} \\ v_{31}&v_{32}\end{bmatrix},\begin{bmatrix}v_{13}&v_{14} \\ v_{23}&v_{24} \\ v_{33}&v_{34} \end{bmatrix}\end{bmatrix}</script><script type="math/tex; mode=display">head_1=attention = softmax(\frac{Q_1K_1^T}{\sqrt{d_k}})V_1</script><script type="math/tex; mode=display">Q_1K_1^T=\begin{bmatrix}q_{11}&q_{12} \\ q_{21}&q_{22} \\ q_{31}&q_{32} \end{bmatrix}\begin{bmatrix}k_{11}&k_{21}&k_{31} \\k_{12}&k_{22}&k_{32}\end{bmatrix}=\begin{bmatrix}z_{11}&z_{12}&z_{13} \\ z_{21}&z_{22}&z_{23} \\ z_{31}&z_{32}&z_{33} \end{bmatrix}(Z_1)</script><script type="math/tex; mode=display">head_1=softmax(\frac{Z_1}{\sqrt{d_k}})V_1=\begin{bmatrix}z^{'}_{11}&z^{'}_{12}&z^{'}_{13} \\ z^{'}_{21}&z^{'}_{22}&z^{'}_{23} \\ z^{'}_{31}&z^{'}_{32}&z^{'}_{33} \end{bmatrix}\begin{bmatrix}v_{11}&v_{12} \\ v_{21}&v_{22} \\ v_{31}&v_{32} \end{bmatrix}=\begin{bmatrix}h^1_{11}&h^1_{12} \\ h^1_{21}&h^1_{22} \\ h^1_{31}&h^1_{32} \end{bmatrix}</script><script type="math/tex; mode=display">head_2=\begin{bmatrix}h^2_{11}&h^2_{12} \\ h^2_{21}&h^2_{22} \\ h^2_{31}&h^2_{32} \end{bmatrix}</script><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head_1,head_2,...)W^o</script><script type="math/tex; mode=display">=[head_1,head_2]W^o=\begin{bmatrix}h^1_{11}&h^1_{12}&h^2_{11}&h^2_{12} \\ h^1_{21}&h^1_{22}&h^2_{21}&h^2_{22} \\ h^1_{31}&h^1_{32}&h^2_{31}&h^2_{32} \end{bmatrix}\begin{bmatrix}w_{11}&w_{12}&w_{13}&w_{14} \\ w_{21}&w_{22}&w_{23}&w_{24} \\ w_{31}&w_{32}&w_{33}&w_{34} \\ w_{31}&w_{32}&w_{33}&w_{34}\end{bmatrix}</script><script type="math/tex; mode=display">=\begin{bmatrix}o_{11}&o_{12}&o_{13}&o_{14} \\ o_{21}&o_{22}&o_{23}&o_{24} \\ o_{31}&o_{32}&o_{33}&o_{34} \end{bmatrix}(seqlen \times dim)</script><hr>
<h4 id="multi-head-attetion-pytorch-Implement"><a href="#multi-head-attetion-pytorch-Implement" class="headerlink" title="multi-head attetion pytorch Implement"></a>multi-head attetion pytorch Implement</h4><pre><code class="lang-python">import torch  
from torch import nn  
import numpy as np

&#39;&#39;&#39;
Self-Attention Shape [batch_size,seq_len,intput_dim]
Multi-Head Attention Shape [batch_size,heads,seq_len,(intput_dim/heads)]
&#39;&#39;&#39;
def attention(query, key, value):  
    key_dim = query.size(-1)  
    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(key_dim)  
    attention_weight = scores.softmax(dim=-1)  
    return torch.matmul(attention_weight, value)

class MultiHeadAttention(nn.Module):  
    def __init__(self, heads, input_dim):  

        super(MultiHeadAttention, self).__init__()  
        assert input_dim % heads == 0  # 可整除  
        self.key_dim = input_dim // heads  
        self.heads = heads  
        self.linears = [  
            nn.Linear(input_dim, input_dim),# W-Q  
            nn.Linear(input_dim, input_dim),# W-K    
            nn.Linear(input_dim, input_dim),# W-V    
            nn.Linear(input_dim, input_dim),# W-O   
        ]  

    def forward(self, x):  
        batch_size = x.size(0)  
        query, key, value = [  
            linear(x).view(batch_size, -1, self.heads, self.key_dim).transpose(1, 2)  
            for linear, x in zip(self.linears, (x, x, x))  
        ]  

        #x shape:[batch_size,heads,seq_len,input_dim/heads]      
        x = attention(query, key, value)  

        #x shape:[batch_size,seq_len,input_dim]      
        x = (x.transpose(1, 2).contiguous().view(batch_size, -1, self.heads * self.key_dim))  
        return self.linears[-1](x)  

mutil_head_model = MultiHeadAttention(2,4)  
output = mutil_head_model(X)  
print(output)  
print(output.size())
</code></pre>
<pre><code class="lang-bash">tensor([[[ 0.1228, -0.1211,  0.5368, -0.1685],
         [ 0.1230, -0.1210,  0.5374, -0.1690],
         [ 0.1229, -0.1207,  0.5375, -0.1683]]], grad_fn=&lt;AddBackward0&gt;)
torch.Size([1, 3, 4])
</code></pre>
<h3 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h3><p><img src="/images/transformer01.jpg" srcset="/img/loading.gif" alt=""></p>
<h4 id="PositionalEncoding"><a href="#PositionalEncoding" class="headerlink" title="PositionalEncoding"></a>PositionalEncoding</h4><blockquote>
<p>[!NOTE]<br>self-attention的无序性丢失了词语的位置信息,Positional Encoding对句中词语的相对位置进行编码,保证词语的有序性</p>
</blockquote>
<p>$[0,1,…,T]$作为位置编码,如第3个词的编码:[3,3,3,3,…,3],当句子过长时最后一个词的值比首词相差较大,合并embedding后易造成特征倾斜,若归一化,即$[0/T,1/T,…,T/T]$作为位置编码可避免特征倾斜问题,但导致不同文本的位置编码步长不一致,如:<br><img src="/images/transformer01.jpg" srcset="/img/loading.gif" alt=""></p>
<blockquote>
<p>[!NOTE] Note<br>利用sin,cos的特性使得位置编码分布处于[0,1]区间,同时位置编码的步长一致,由于其周期性会导致不同pos而位置编码一致的情况,因此 pos/X 可试周期无限长并交替使用sin和cos,从而避免</p>
</blockquote>
<script type="math/tex; mode=display">posCode_{(pos,2i)}=sin(\frac{pos}{10000^{\frac{2i}{dim}}})</script><script type="math/tex; mode=display">posCode_{(pos,2i+1)}=cos(\frac{pos}{10000^{\frac{2i}{dim}}})</script><p>$pos$ : 单词所在句中位置<br>$2i$ : 偶数维度<br>$2i+1$ : 奇数维度<br>$dim$ : 编码维度<br>$X=[0.1,0.2,0.3,0.4],pos=2,posCode=[sin(\frac{2}{10000^{\frac{0}{4}}}),cos(\frac{2}{10000^{\frac{0}{4}}}),sin(\frac{2}{10000^{\frac{2}{4}}}),cos(\frac{2}{10000^{\frac{2}{4}}})]$ </p>
<pre><code class="lang-python">class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout: float = 0.1, max_len: int = 64):
        super().__init__()

        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))

        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)

        self.register_buffer(&#39;pe&#39;, pe)

    def forward(self, x: Tensor) -&gt; Tensor:
        #x: Tensor, shape [seq_len, batch_size, embedding_dim]
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)
</code></pre>
<h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><script type="math/tex; mode=display">X_{n \times d}=PositionalEncoding(X)</script><script type="math/tex; mode=display">FeedForward(x)=max(0,xw_1+b_1)w_2+b_2</script><script type="math/tex; mode=display">output'=LayerNorm(X+multiHeadAttention(X))</script><script type="math/tex; mode=display">output_{n \times d}=LayerNorm(output'+FeedForward(output'))</script><h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><script type="math/tex; mode=display">Y_{n \times d}=PositionalEncoding(Y)</script><script type="math/tex; mode=display">query=LayerNorm(Y+maskedMultiHeadAttention(Y))</script><script type="math/tex; mode=display">output'=LayerNorm(query+multiHeadAttention(encoderOutput,encoderOutput,query))</script><script type="math/tex; mode=display">output=LayerNorm(output'+FeedForward(output'))</script><script type="math/tex; mode=display">output=Softmax(Linear(output))</script><p>$Y:shifted-right-output$<br>note : 我有一只猫 -&gt; I have a cat<br>$Y$:[begin] -&gt; I,[begin,I]-&gt;have,[begin,I,have]-&gt;a,[begin,I have a ]-&gt;cat</p>
<pre><code class="lang-python">class FeedForward(nn.Module):  
    def __init__(self, d_model, d_ff, dropout=0.1):  
        super(PositionwiseFeedForward, self).__init__()  
        self.w_1 = nn.Linear(d_model, d_ff)  
        self.w_2 = nn.Linear(d_ff, d_model)  
        self.dropout = nn.Dropout(dropout)  

    def forward(self, x):  
        return self.w_2(self.dropout(self.w_1(x).relu()))
</code></pre>
<h5 id="maskedMultiHeadAttention"><a href="#maskedMultiHeadAttention" class="headerlink" title="maskedMultiHeadAttention"></a>maskedMultiHeadAttention</h5><p><img src="/images/transformer03.jpg" srcset="/img/loading.gif" alt=""><br><img src="/images/transformer04.jpg" srcset="/img/loading.gif" alt=""><br><img src="/images/transformer05.jpg" srcset="/img/loading.gif" alt=""><br><img src="/images/transformer06.jpg" srcset="/img/loading.gif" alt=""></p>
<blockquote>
<p>[!NOTE] Note<br>train时经masked后传入整个输入矩阵等同于推理时按词顺序挨个推理</p>
</blockquote>
<pre><code class="lang-python">def generate_square_subsequent_mask(size: int) -&gt; Tensor:  
    #size:
    return torch.triu(torch.full((size, size), float(&#39;-inf&#39;)), diagonal=1)    

def attention(query, key, value, mask=None, dropout=None):   
    d_k = query.size(-1)  
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  
    if mask is not None:  
        scores = scores.masked_fill(mask == 0, -1e9)  
    p_attn = scores.softmax(dim=-1)  
    if dropout is not None:  
        p_attn = dropout(p_attn)  
    return torch.matmul(p_attn, value), p_attn
</code></pre>
<h4 id="transformer-implement"><a href="#transformer-implement" class="headerlink" title="transformer implement"></a>transformer implement</h4><pre><code class="lang-python">#waiting
</code></pre>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;TOC</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
      <br><br>
      
      
  <script defer src="https://utteranc.es/client.js"
          repo="JiaoPaner/blog-talk"
          issue-term="pathname"
  
          label="utterances"
    
          theme="github-light"
          crossorigin="anonymous"
          async>
  </script>


    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    


    <!-- cnzz Analytics icon -->
    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>



  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  <!-- cnzz Analytics -->
  



  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "self-attention|transformer&nbsp;",
      ],
      cursorChar: ".",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>



  

  
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
              processEscapes: true,
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
          }
      });

      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });

    </script>

    <script src="https://cdn.staticfile.org/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML" ></script>

  









</body>
</html>
